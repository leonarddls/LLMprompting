|**Framework Name**|**Factors Considered for Evaluation**|**Url Link**|
|---|---|---|
|Big Bench|Generalization abilities|https://github.com/google/BIG-bench|
|GLUE Benchmark|Grammar, Paraphrasing, Text Similarity, Inference, Textual Entailment, Resolving Pronoun References|https://gluebenchmark.com/|
|SuperGLUE Benchmark|Natural Language Understanding, Reasoning, Understanding complex sentences beyond training data, Coherent and Well-Formed Natural Language Generation, Dialogue with Human Beings, Common Sense Reasoning (Everyday Scenarios and Social Norms and Conventions), Information Retrieval, Reading Comprehension|https://super.gluebenchmark.com/|
|OpenAI Moderation API|Filter out harmful or unsafe content|https://platform.openai.com/docs/api-reference/moderations|
|MMLU|Language understanding across various tasks and domains|https://github.com/hendrycks/test|
|EleutherAI LM Eval|few-shot evaluation and performance in a wide range of tasks with minimal fine-tuning|https://github.com/EleutherAI/lm-evaluation-harness|
|OpenAI Evals|Accuracy, Diversity, Consistency, Robustness, Transferability, Efficiency, Fairness of text generated|https://github.com/openai/evals|
|Adversarial NLI (ANLI)|Robustness, Generalization, Coherent explanations for inferences, Consistency of reasoning across similar examples, Efficiency in terms of resource usage (memory usage, inference time, and training time)|https://github.com/facebookresearch/anli|
|LIT (Language Interpretability Tool)|Platform to Evaluate on User Defined Metrics. Insights into their strengths, weaknesses, and potential biases|https://pair-code.github.io/lit/|
|ParlAI|Accuracy, F1 score, Perplexity (how well the model predicts the next word in a sequence), Human evaluation on criteria like relevance, fluency, and coherence, Speed & resource utilization, Robustness (this evaluates how well the model performs under different conditions such as noisy inputs, adversarial attacks, or varying levels of data quality), Generalization|https://github.com/facebookresearch/ParlAI|
|CoQA|understand a text passage and answer a series of interconnected questions that appear in a conversation.|https://stanfordnlp.github.io/coqa/|
|LAMBADA|Long-term understanding using prediction of the last word of a passage.|https://zenodo.org/record/2630551#.ZFUKS-zML0p|
|HellaSwag|Reasoning abilities|https://rowanzellers.com/hellaswag/|
|LogiQA|Logical reasoning abilities|https://github.com/lgw863/LogiQA-dataset|
|MultiNLI|Understanding relationships between sentences across different genres|https://cims.nyu.edu/~sbowman/multinli/|
|SQUAD|Reading comprehension tasks|https://rajpurkar.github.io/SQuAD-explorer/|

